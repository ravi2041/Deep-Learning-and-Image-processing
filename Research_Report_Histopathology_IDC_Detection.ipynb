{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research Report - Histopathology IDC Detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "u1LkIvkCgVmX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "3LBFMDb2k22Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DATA 472 Research Project**\n",
        "\n",
        "**Research team - Ravi Kumar Singh (12159093) ,rks55@uclive.ac.nz and\n",
        "                                       Dhruv Sharma (76749332), dsx11@uclive.ac.nz **\n",
        "\n",
        "Research Project Domain- Machine Learning & Image Processing.\n",
        "\n",
        "Medical Domain- Histopathology tumor detection\n",
        "\n",
        "---\n",
        "Project Proposal Link - **https://docs.google.com/document/d/12j1Thz56FcMgTFwN4qV2Nb5CUeoNd4h8YBlGFoqUEZw/edit?usp=sharing**\n",
        "\n",
        "Parent Research paper Link- \n",
        "**https://www.researchgate.net/publication/263052166_Automatic_detection_of_invasive_ductal_carcinoma_in_whole_slide_images_with_Convolutional_Neural_Networks\n",
        "**"
      ]
    },
    {
      "metadata": {
        "id": "smf2t2mZJHKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "---\n",
        "IDC is the most phenotypic subtype of all the breast cancers (BCa) comprising nearly 80% of them. IDC is usually identified by pathologists through visual analysis of tissue slides stained with H&E (Hematoxylin & \n",
        "Eosin). Through this research, focus was to improve the existing CNN results and to further explore whether different color spaces help improve the classification recall score.If yes, which color based feature spaces amongst the four -HSV, RGB, YUV or YCRCB,when used alongwith basic CNN and pre trained VGG 16 CNN models would perform the best in classifying a patch containing IDC or not.Our results show that RGB and YUV gave us the best recall score. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "01g_n8WfJHBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ]
    },
    {
      "metadata": {
        "id": "Ssi3UjBBJG6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "---\n",
        "\n",
        "The analysis of histopathological image is done manually by a pathologist and therefore the diagnosis is subjective and greatly dependent on the level of expertise of the professional. Breast cancer is the most frequent type of cancer in women. It is estimated that one in 10 women may suffer from it sometime during their life. Although a large majority of breast cancer tumours are cured, even today 20% of patients are not. Resistance to treatment or the absence of specific therapies against certain types of breast cancer explains why certain tumours cannot be completely eliminated.The parent paper presents a deep learning approach for automatic detection and visual analysis of invasive ductal carcinoma (IDC) tissue regions in whole slide images (WSI) of breast cancer (BCa). Deep learning approaches are learn-from-data methods involving computational modeling of the learning process.\n",
        "\n",
        "The original paper presented a deep learning framework for automated detection of IDC regions in WSI of BCa histopathology. They compared their result for vanilla implementation of CNN  (containg  2 convolutional layers, 2 subsampling layers and 1 fully connexted layers) with different visual feature extractor techniques including tissue morphological features like nuclear texture and nuclear architecture, different color histograms etc which were run on Random Forest classifier.Their results suggested that only two color based fature extractors were able to accurately identify IDC using random forests while their vanilla implementation of CNN performed the best. \n",
        "\n",
        "Through this research, we are focussed to explore which color based feature spaces  amongst the four -HSV, RGB, YUV or YCRCB,when used alongwith CNN, would perform the best in identifying a patch containing IDC or not. \n",
        " \n",
        " **Why are we focusing on colour based feature extractors?**\n",
        "\n",
        "> More representative of how a pathologist visually identifies IDC at lower resolutions.\n",
        "\n",
        "> Tissue Morphological features like nuclear texture and nuclear architecture are more important in distinguishing between different grades of IDC.\n",
        "\n",
        "> Our analysis is only limited to identify a patch containing IDC or not (which could later be extented to diferent grades of  cancer)"
      ]
    },
    {
      "metadata": {
        "id": "IQvoNNbiJGqN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset Summary\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset Link -** https://www.kaggle.com/paultimothymooney/breast-histopathology-images\n"
      ]
    },
    {
      "metadata": {
        "id": "SKdoBdtqXGdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "162 Women diagnosed with IDC at the Hospital of University of Pennsylvania and the Cancer Institute of New Jersey. Dataset available on Kaggle Link.Whole Slide Images were digitised with a whole slide scanner at 40X magnification. The dataset provides **each patch of size 50X50pixels ** (unlike 81X81 pixel sized images in original paper)\n",
        "\n",
        "Patch Counts - \n",
        "\n",
        "1. Train Patches - 161909\n",
        "2. Validation Patches - 49581\n",
        "3. Test Patches – 66034\n",
        "\n",
        "Class 0 means the patch does not contain IDC and Class 1 means the patch contains IDC. Class Balance Ratio – IDC : Non IDC :: 3 : 1\n"
      ]
    },
    {
      "metadata": {
        "id": "jeLc0AFIf7ix",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###IDC Dataset - Google Drive link\n",
        "\n",
        "---\n",
        "\n",
        "We have divided our dataset into three mainly training, validation and testing.  We have maintained a directory hierarchy in this order:\n",
        "Train -> subclass (class 0 and class 1) where class 0 contains NON-IDC(sample without cancer) data and class 1 IDC(Sample with cancer) data.\n",
        "We uploaded entire data into google drive and used their API to upload data into Colab. The below function does exactly the same. "
      ]
    },
    {
      "metadata": {
        "id": "aTJxQq2lGg3e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "import pandas as pd\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_ds2 = drive.CreateFile({'id': '18s7K23jhSsuB8ILl2CWsAuWAxAS_n-BO'})\n",
        "file_ds2.GetContentFile('idc_dataset.zip')\n",
        "  \n",
        "# !rm -rf dataset-master\n",
        "!rm -rf idc_dataset  \n",
        "\n",
        "# !unzip -q dataset.zip\n",
        "!unzip -q idc_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRQH7w9RJGdG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hypothesis"
      ]
    },
    {
      "metadata": {
        "id": "rpUY1p40N2u2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Research Hypothesis-**\n",
        "By using different visual property based global feature extractors specifically color based feature extractors  along with the usage of deep neural networks would increase the recall score by at least five percent as compared to the existing CNN recall score.\n",
        "\n",
        "**Color Spaces to be tried in the experiment** -\n",
        "\n",
        "> YUV - Common in image and video compression.It is half the size of RGB and can be processed twice as fast using CNN.\n",
        "\n",
        ">  HSV is hue, saturation and Value. HSV is derived from RGB colour space. HSV rearranges the geometry of RGB in an attempt to be more intuitive and perceptually relevant.\n",
        "\n",
        "> RGB - The RGB colour model is used on Computer Monitors and Television screens to produce the colour that the user will see when using the system.\n",
        "\n",
        ">YCRCB - Luminance Chroma Blue Chroma Red. \n",
        "\n",
        "**Expected Results**\n",
        "Since the ground truth annotations were done by a human pathologist using RGB images, expecting to see that RGB performs best. Also because we believe image loses information when converted to other colour spaces.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-NomPdLbJglF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimental Design and Methods"
      ]
    },
    {
      "metadata": {
        "id": "bfhoeADEOP04",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Architecture Diagram - https://docs.google.com/drawings/d/1ai1ABlKJuPErCM7ZTWyVTFA0nGUTCXNuY1H_yx7Yi-E/edit?usp=sharing\n",
        "\n",
        "---\n",
        "\n",
        "As descibed in the process flow diagram, we experiment using two strategy approach. In strategy one, we build the model using basin CNN and try to imitate the design from original paper. In Strategy two we build pre trained VGG 16 models using the four color spaces.We do perform normalisation on the images in strategy1. Intention is to run the eight models on different thresholds and then compare the recall values we get from the confusion matrix after running the model on the test set."
      ]
    },
    {
      "metadata": {
        "id": "q-q_6Sq5KmTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Performace Metrics\n",
        "\n",
        "---\n",
        "\n",
        "**Model assessment done using Confusion Matrix  and ROC using testing data and Classification Report.**\n",
        "\n",
        "> **True Positive (TP):** Number of observations classified as true and are actually true.\n",
        "> **False Positive (FP):** Number of observations classified as true and are actually false.\n",
        "> **False Negative(FN):** Number of observations classified as negative and are actually true.\n",
        "> **True Negative(TN):** Number of observations classified as false and are actually false.\n",
        "> **Accuracy:** Number of correct predictions out of the total number of observations = (TP+TN)/(TP+TN+FP+FN)\n",
        "> **Precision:** Number of items correctly identified as positive out of total items identified as positive=TP/(TP+FP)\n",
        "> **Recall or sensitivity:** Number of items correctly identified as positive out of total true positives = TP/(TP+FN)\n",
        "> **Specificity:** Number of items correctly identified as negative out of total negative = TN/(TN+FP)\n",
        "\n",
        "We have concentrated on **Recall** score as we are aiming to reduce **False negitives** and increase **true positives.** Recall is the proportion of IDC correctly predicted from whole IDC automatically predicted.\n",
        "\n",
        "** Recall – TP / TP + FN where, TP = Patches having IDC and detected correctly and FN = Patches having IDC but not detected correctly.**\n",
        "\n",
        "**We ran our results on 4 threshold values - 0.29, 0.4, 0.5, 0.7 . Best results were obtained at 0.29 threshold.**"
      ]
    },
    {
      "metadata": {
        "id": "PJyKO89BXkco",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Tuning and Image Pre processing\n",
        "\n",
        "---\n",
        "\n",
        "1. **Data Augmentation:** was performed using Keras Image Data Generator function for both CNN and VGG 16 implementations. Sometimes images in the sample data may have varying and different rotations in the scene. Hence we trained our model to better handle rotations of images by artificially and randomly rotating images during training. Also, images may be off-center in a variety of different ways.We performed Image Generator capabilities such as Random rotation, shifts, shear and flips.\n",
        "\n",
        "2. **Normalisation:** As part of normalisation strategy for our dataset, we did histogram equalisation. The exposure equalisation histogram function is available from the scikit-image library in python.The equalized image has a roughly linear cumulative distribution function.The equalised images were only passed to CNN basic models and not the VGG 16 models. This was because for basic CNN implementation we wanted to imitate the procedure performed and results obtained in the original parent paper.\n",
        "\n",
        "3. **Cost, Decay and Thresholds optimisation:**  The original parent paper had achieved the best results for threshold value  at 0.29. Parameter tuning was performed at learning rate equal to 0.01 and decay rate at 0.0000001. In our implementation, we have trained he model using learning rate as 0.01 and decay rate as 0.007 and followed the same optimiser Stochastic gradient descent being used in the paper. \n",
        "We checked our results at **four** different threshold values and compared against the original paper. \n",
        "\n",
        "4. **Train-validate-Test Approach with 10 epochs:**  The train validate test split was done in 60:20:20 ratio approximately. The training and validation was performed with **epochs equal to 10.** The best results in the original paper were achieved at epochs = 25, but due to huge number of  images and in the interest of savng time to train each model, we standardised epoch size equal to 10. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "aifs0WbECom0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Basic CNN Implementation"
      ]
    },
    {
      "metadata": {
        "id": "AQc5ezK_LeoP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Importing Libraries"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iX4byUg9DEYn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import sklearn\n",
        "import itertools\n",
        "import cv2\n",
        "import scipy\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "from skimage import exposure\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u7ntjzMCDpB-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Images in different Color Schemes"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2YBwX3CQJVHg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "plt.figure(figsize=(2,2))\n",
        "#new_image = cv2.cvtColor(image)\n",
        "plt.imshow(image)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jL0MB51mJlqm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "plt.figure(figsize=(2,2))\n",
        "new_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
        "plt.imshow(new_image)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4k6Iyxt2-E3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "plt.figure(figsize=(2,2))\n",
        "new_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
        "#hist = cv2.calcHist( [new_image], [0, 1], None, [180, 256], [0, 180, 0, 256] )\n",
        "plt.imshow(new_image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NHx-kavQJ1Lw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "plt.figure(figsize=(2,2))\n",
        "new_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "#hist = cv2.calcHist( [new_image], [0, 1], None, [180, 256], [0, 180, 0, 256] )\n",
        "plt.imshow(new_image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fda4fZ4NG163",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####  Image Pre processing - Image Equalisation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HEJsuzc0IwbA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YUV Image\n",
        "image = cv2.imread(\"idc_dataset/test/class0/class1_49.png\")\n",
        "yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2YUV)\n",
        "image = exposure.equalize_hist(yuv_image)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "INrwqePcImNZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Eqalisation performed\n",
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
        "image = exposure.equalize_hist(yuv_image)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bmAzpzi-IMee",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Eqalisation performed\n",
        "image = cv2.imread(\"idc_dataset/test/class1/class1_49.png\")\n",
        "yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2YCrCb)\n",
        "image = exposure.equalize_hist(yuv_image)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qX4kpbgmxk0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cnn_model():\n",
        "    model = Sequential([\n",
        "        Convolution2D(16,(5,5), activation='tanh', input_shape = (50,50,3)),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Convolution2D(32,(5,5), activation='tanh'),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Convolution2D(128,(5,5), activation='tanh'),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.25),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.25),\n",
        "        Dense(2, activation='softmax')\n",
        "        ])\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=0.01, decay= 0.0007),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dQiDwPo-5J7I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def thresholds_function(model_name):\n",
        "  threshold_list3 = [0.29,0.4,0.5,0.7]\n",
        "  num_of_test_samples = 66029\n",
        "  batch_size = 32\n",
        "  for i in threshold_list3:\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    Y_pred = model_name.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "    y_pred = (Y_pred > i).astype(int)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    print('Confusion Matrix for', i)\n",
        "    print(confusion_matrix(test_generator.classes, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAwJUwBAHuaC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### CNN Model for YUV Color space"
      ]
    },
    {
      "metadata": {
        "id": "SNH6EoNOOoeB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_yuv = get_cnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0qNZ2imO2Y7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "    \n",
        "def myFunc(image):\n",
        "    yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2YUV)\n",
        "    image = exposure.equalize_hist(image)\n",
        "    return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        #rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(#rescale=1./255,\n",
        "                                 preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'idc_dataset/train/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "      'idc_dataset/validation/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/test/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "    shuffle = False,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "cnn_yuv.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        workers = 12\n",
        "    ,\n",
        "       validation_data=validation_generator\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCPX51h1IR4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#####Confusion Matrix at 4 different Thresholds"
      ]
    },
    {
      "metadata": {
        "id": "QIBzyP7EOmC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold_list1 = [0.29,0.4,0.5,0.7]\n",
        "num_of_test_samples = 66029\n",
        "batch_size = 32\n",
        "for i in threshold_list1:\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  Y_pred = cnn_yuv.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = (Y_pred > i).astype(int)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  print('Confusion Matrix for', i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVhC47dPIcye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### AUROC for YUV Color Space"
      ]
    },
    {
      "metadata": {
        "id": "_sDt-qxmMtwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66029\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = cnn_yuv.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = (Y_pred > i).astype(int)\n",
        "      y_pred = np.argmax(y_pred, axis=1)\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for YUV CNN (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for YUV')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for YUV CNN (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciC2OJ74IpCJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### CNN Model for RGB color Space"
      ]
    },
    {
      "metadata": {
        "id": "xeUs9CvdeOe4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rgb_model = get_cnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9LD6CyAeOiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "\n",
        "def myFunc(image):\n",
        "    image = exposure.equalize_hist(image)\n",
        "    return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        #rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    #rescale=1./255,\n",
        "    preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'idc_dataset/train/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/validation/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/test/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "    shuffle = False,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "rgb_model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        workers = 12\n",
        "    ,\n",
        "        validation_data=validation_generator\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IzgzsFmIuoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix at 4 Thresholds"
      ]
    },
    {
      "metadata": {
        "id": "qP6vtN2WRccy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold_list2 = [0.29,0.4,0.5,0.7]\n",
        "num_of_test_samples = 66029\n",
        "batch_size = 32\n",
        "for i in threshold_list2:\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  Y_pred = rgb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = (Y_pred > i).astype(int)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  print('Confusion Matrix for', i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0vZCUrywI07c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### ROC AUC Curve for RGB Model"
      ]
    },
    {
      "metadata": {
        "id": "QaGKMJDL4ETl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66029\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = rgb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = (Y_pred > i).astype(int)\n",
        "      y_pred = np.argmax(y_pred, axis=1)\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for RGB CNN (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for RGB')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for RGB CNN (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_p_DI2x5JcVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####CNN Model for YCRCB Color Space"
      ]
    },
    {
      "metadata": {
        "id": "OTP7R0nqt9Cd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ycrcb_model = get_cnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a_Q1mlcmudBZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "    \n",
        "def myFunc(image):\n",
        "    \n",
        "    RGB2YCrCb_image = cv2.cvtColor(image,cv2.COLOR_RGB2YCrCb)\n",
        "    \n",
        "    image = exposure.equalize_hist(RGB2YCrCb_image)\n",
        "    return image\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        #rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    #rescale=1./255,\n",
        "                                 preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'idc_dataset/train/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/validation/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/test/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "    shuffle = False,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "ycrcb_model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        workers = 12\n",
        "    ,\n",
        "        validation_data=validation_generator\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbZXxDDaJr3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix"
      ]
    },
    {
      "metadata": {
        "id": "psQWFvXDVinv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold_list3 = [0.29,0.4,0.5,0.7]\n",
        "num_of_test_samples = 66029\n",
        "batch_size = 32\n",
        "for i in threshold_list3:\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  Y_pred = ycrcb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = (Y_pred > i).astype(int)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  print('Confusion Matrix for', i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-iq9n_UJyh2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### ROC AUC Curve for YCRCB Model"
      ]
    },
    {
      "metadata": {
        "id": "ZVlzZ8MWXAUQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66029\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = ycrcb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = (Y_pred > i).astype(int)\n",
        "      y_pred = np.argmax(y_pred, axis=1)\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for YCRCB CNN (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for YCRCB CNN')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for YCRCB CNN (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oc6-6RwSJ_q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### CNN Model for HSV Color Space"
      ]
    },
    {
      "metadata": {
        "id": "EDn4LRGGe-14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cnn_model():\n",
        "    model = Sequential([\n",
        "        Convolution2D(16,(5,5), activation='tanh', input_shape = (50,50,3)),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Convolution2D(32,(5,5), activation='tanh'),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Convolution2D(128,(5,5), activation='tanh'),\n",
        "        MaxPooling2D(pool_size= (2,2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.25),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.25),\n",
        "        Dense(2, activation='softmax')\n",
        "        ])\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=0.01, decay= 0.0007),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9CDtHqceWt9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hsv_model = get_cnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V31dnu1KXGzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def myFunc(image):\n",
        "    image = np.array(image)\n",
        "    yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
        "    img_arr = np.asarray(yuv_image)\n",
        "    return img_arr\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                 preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'idc_dataset/train/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/validation/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'idc_dataset/test/',\n",
        "        target_size=(50, 50),\n",
        "        batch_size=32,\n",
        "    shuffle = False,\n",
        "        class_mode= 'categorical')\n",
        "\n",
        "hsv_model.fit_generator(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        workers = 12\n",
        "    ,\n",
        "        validation_data=validation_generator\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XaY0ItbDKO-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix"
      ]
    },
    {
      "metadata": {
        "id": "lB9ooBMle4-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold_list3 = [0.29,0.4,0.5,0.7]\n",
        "num_of_test_samples = 66029\n",
        "batch_size = 32\n",
        "for i in threshold_list3:\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  Y_pred = hsv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = (Y_pred > i).astype(int)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  print('Confusion Matrix for', i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzTJqS58J9w1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### ROC AUC Curve for HSV Color Space"
      ]
    },
    {
      "metadata": {
        "id": "U1KmzPtt40ln",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66029\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = hsv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = (Y_pred > i).astype(int)\n",
        "      y_pred = np.argmax(y_pred, axis=1)\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for HSV CNN (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for HSV')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for HSV CNN (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EtQLEW4QJkzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##VGG16 Models"
      ]
    },
    {
      "metadata": {
        "id": "u1LkIvkCgVmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries "
      ]
    },
    {
      "metadata": {
        "id": "9wMvh-lJkfX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xml.etree.ElementTree as ET\n",
        "import sklearn\n",
        "import itertools\n",
        "import cv2\n",
        "import scipy\n",
        "import os\n",
        "import csv\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras import applications\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.metrics import classification_report, confusion_matrix,f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQaHxYdJgd07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### VGG 16 Architecture "
      ]
    },
    {
      "metadata": {
        "id": "tan65SvpKFGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "VGG known as Visual Geometry Group is deveolped in oxford University. VGG 16 has total 16 layers in it. The archtiecture starts by default input shape which is 224x224x3. Filter or feature detector used is 3x3 matrix with 1 stride. First layer starts with 64 feature maps and it is increased  to 128, 256 and last is 512. Maxpool is 2x2 matrix with 2 strides which is used for extracting important features and same time it reduces the size of the images with minimal loss of information.Each layer has relu activation link. Flatten layer has 4096 nodes. Size of VGG 16 is 533 MB with pretrained weights. Number of learned parameters increases with each layers. Imagenet is used for pretrained weights. It is easy to implement and they are supported by keras so it can be easily modified for various image classification purpose."
      ]
    },
    {
      "metadata": {
        "id": "n1w0yhPZDzfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "model_vgg16_conv.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7UQKuWGfpjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### YUV COLOR SPACE USING VGG 16 Network"
      ]
    },
    {
      "metadata": {
        "id": "UawYcCeSMgP3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First model we trained is YUV model. We converted RGB patches to YUV color space. Each image is 50x50 pixels so we changed the default input parameter of VGG 16 to our specified input shape. We made three variables for train ,validation and testing. \n",
        "We made a function that converts each image color sapce into desired color space. Number of epochs our model runs is 10. We can increase the epoch but it takes more time to run, so we settled with 10 epochs. Batch size is 32 to counter the memory issue as we have lots of images to be trained. On top of VGG 16 network we have created two more Dense layers. Our last layer is sigmoid which is used for binary classification.\n",
        "The hyper parameter tuning :\n",
        "loss function - binary crossentropy\n",
        "optimizer - Stochastic Gradient Descent with learning rate 0.01\n",
        "metric - accuracy and roc auc\n",
        "Next step is data augmentation:  we have used Keras image data generator with shear range =0.2, zoom range=0.2  and horizontal flip set to True and passed our colour conversion function to image data generator. We have rescaled each image of train, validation and test data. \n",
        "Each generator retrieves images from respective directory which they are assigned. Data is shuffled while reading and  for test data we have put false so that we dont encounter wrong labels for images while testing\n",
        "This is the main idea of our modelling and we have tried with same parameter tuning and same setup with different colour spaces which we will see further."
      ]
    },
    {
      "metadata": {
        "id": "pjssSOUuaQG1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model for HSV Color Space\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 50, 50\n",
        "\n",
        "\n",
        "train_data_dir = 'idc_dataset/train/'\n",
        "validation_data_dir = 'idc_dataset/validation/'\n",
        "test_data_dir = 'idc_dataset/test/'\n",
        "nb_train_samples = 3519\n",
        "nb_validation_samples = 2272\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "def myFunc(image):\n",
        "    image = np.array(image)\n",
        "    yuv_image = cv2.cvtColor(image,cv2.COLOR_RGB2YUV)\n",
        "    img_arr = np.asarray(yuv_image)\n",
        "    return img_arr\n",
        "\n",
        "  \n",
        "def auc_roc(y_true, y_pred):\n",
        "    # any tensorflow metric\n",
        "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
        "\n",
        "    # find all variables created for this metric\n",
        "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
        "\n",
        "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
        "    # They will be initialized for new session.\n",
        "    for v in metric_vars:\n",
        "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
        "\n",
        "    # force to update metric values\n",
        "    with tf.control_dependencies([update_op]):\n",
        "        value = tf.identity(value)\n",
        "        return value\n",
        "      \n",
        "#Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "#model_vgg16_conv.summary()\n",
        "\n",
        "#Create our own input format\n",
        "input = Input(shape=(50,50,3),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(1, activation='sigmoid',name= 'predictions')(x)\n",
        "\n",
        "\n",
        "#Create your own model \n",
        "yuv_model = Model(input=input, output=x)\n",
        "\n",
        "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
        "yuv_model.summary()\n",
        "\n",
        "yuv_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-2),\n",
        "              metrics=['accuracy',auc_roc])\n",
        "\n",
        "my_callbacks = [EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')]\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1. / 255,\n",
        "    preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle= False,\n",
        "    class_mode='binary')\n",
        "# fine-tune the model\n",
        "yuv_model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=my_callbacks,\n",
        "    nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UB9ay2pKSAXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#####Confusion Matrix and Classification Report for YUV model"
      ]
    },
    {
      "metadata": {
        "id": "wiSq-6igUExk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have 66034 test samples. We tested our model on different thresholds to find the best Recall Value as we are more interested in finding patients having cancer rather than making a classifier which gives good accuracy but misses on main fundamental point of finding cancer in tissues."
      ]
    },
    {
      "metadata": {
        "id": "6ZeONKIUhyg6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_of_test_samples = 66034\n",
        "batch_size = 32\n",
        "list1 = [0.2, 0.29,0.4,0.5,0.7]\n",
        "for i in list1:\n",
        "  Y_pred = yuv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1,verbose = 1)\n",
        "  y_pred = Y_pred > i\n",
        "  #y_pred = np.argmax(Y_pred, axis=1)\n",
        "  print('Confusion Matrix ',i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7rORqBpYols",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#####ROC AUC Curve for YUV Model"
      ]
    },
    {
      "metadata": {
        "id": "tykqXpm9BNvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  from sklearn.metrics import auc, roc_curve\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66034\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = yuv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = Y_pred > i\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for YUV VGG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for YUV VGG')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for YUV VGG (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCRh-bI8fhfj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###HSV COLOR SPACE USING VGG 16 Network"
      ]
    },
    {
      "metadata": {
        "id": "-nPqUNnrOMsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model for HSV Color Space\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 50, 50\n",
        "\n",
        "\n",
        "train_data_dir = 'idc_dataset/train/'\n",
        "validation_data_dir = 'idc_dataset/validation/'\n",
        "test_data_dir = 'idc_dataset/test/'\n",
        "nb_train_samples = 3519\n",
        "nb_validation_samples = 2272\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "def myFunc(image):\n",
        "    image = np.array(image)\n",
        "    hsv_image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
        "    img_arr = np.asarray(hsv_image)\n",
        "    return img_arr\n",
        "\n",
        "  \n",
        "def auc_roc(y_true, y_pred):\n",
        "    # any tensorflow metric\n",
        "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
        "\n",
        "    # find all variables created for this metric\n",
        "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
        "\n",
        "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
        "    # They will be initialized for new session.\n",
        "    for v in metric_vars:\n",
        "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
        "\n",
        "    # force to update metric values\n",
        "    with tf.control_dependencies([update_op]):\n",
        "        value = tf.identity(value)\n",
        "        return value\n",
        "      \n",
        "#Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "#model_vgg16_conv.summary()\n",
        "\n",
        "#Create your own input format\n",
        "input = Input(shape=(50,50,3),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(1, activation='sigmoid',name= 'predictions')(x)\n",
        "\n",
        "\n",
        "#Create your own model \n",
        "hsv_model = Model(input=input, output=x)\n",
        "\n",
        "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
        "hsv_model.summary()\n",
        "\n",
        "hsv_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-2),\n",
        "              metrics=['accuracy',auc_roc])\n",
        "\n",
        "my_callbacks = [EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')]\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1. / 255,\n",
        "    preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle= False,\n",
        "    class_mode='binary')\n",
        "# fine-tune the model\n",
        "hsv_model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=my_callbacks,\n",
        "    nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmuP9pCrSU9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix and Classification Report for HSV model"
      ]
    },
    {
      "metadata": {
        "id": "5qnCdkMKijBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,f1_score\n",
        "num_of_test_samples = 66034\n",
        "batch_size = 32\n",
        "list1 = [0.29,0.4,0.5,0.7]\n",
        "for i in list1:\n",
        "  Y_pred = hsv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = Y_pred > i\n",
        "  print('Confusion Matrix')\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9fu42WIiSgMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### ROC AUC Curve For HSV Model"
      ]
    },
    {
      "metadata": {
        "id": "Cg9ihazMAoJm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  from sklearn.metrics import auc, roc_curve\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66034\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = hsv_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = Y_pred > i\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for HSV VGG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for HSV VGG')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for HSV VGG (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ocINo4ufWL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### YCBCR COLOR SPACE USING VGG 16 Network"
      ]
    },
    {
      "metadata": {
        "id": "xf9GIZBiWPQ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model for YCBCR Color Space\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 50, 50\n",
        "\n",
        "\n",
        "train_data_dir = 'idc_dataset/train/'\n",
        "validation_data_dir = 'idc_dataset/validation/'\n",
        "test_data_dir = 'idc_dataset/test/'\n",
        "nb_train_samples = 3519\n",
        "nb_validation_samples = 2272\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "def myFunc(image):\n",
        "    image = np.array(image)\n",
        "    ycrcb_image = cv2.cvtColor(image,cv2.COLOR_RGB2YCrCb)\n",
        "    img_arr = np.asarray(ycrcb_image)\n",
        "    return img_arr\n",
        "\n",
        "  \n",
        "def auc_roc(y_true, y_pred):\n",
        "    # any tensorflow metric\n",
        "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
        "\n",
        "    # find all variables created for this metric\n",
        "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
        "\n",
        "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
        "    # They will be initialized for new session.\n",
        "    for v in metric_vars:\n",
        "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
        "\n",
        "    # force to update metric values\n",
        "    with tf.control_dependencies([update_op]):\n",
        "        value = tf.identity(value)\n",
        "        return value\n",
        "      \n",
        "#Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "#model_vgg16_conv.summary()\n",
        "\n",
        "#Create your own input format\n",
        "input = Input(shape=(50,50,3),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(1, activation='sigmoid',name= 'predictions')(x)\n",
        "\n",
        "\n",
        "#Create your own model \n",
        "ycrcb_model = Model(input=input, output=x)\n",
        "\n",
        "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
        "ycrcb_model.summary()\n",
        "\n",
        "ycrcb_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-2),\n",
        "              metrics=['accuracy',auc_roc])\n",
        "\n",
        "my_callbacks = [EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')]\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1. / 255,\n",
        "    preprocessing_function = myFunc)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function = myFunc)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle= False,\n",
        "    class_mode='binary')\n",
        "# fine-tune the model\n",
        "ycrcb_model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=my_callbacks,\n",
        "    nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ajVCkaoTD8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix and Classification Report"
      ]
    },
    {
      "metadata": {
        "id": "4XUYi23ZcltB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,f1_score\n",
        "num_of_test_samples = 66034\n",
        "batch_size = 32\n",
        "#y_true = np.array([0] * 1000 + [1] * 1000)\n",
        "list1 = [0.29,0.4,0.5,0.7]\n",
        "for i in list1:\n",
        "  Y_pred = ycrcb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = Y_pred > i\n",
        "  # y_pred = np.argmax(Y_pred, axis=1)\n",
        "  # print(y_pred)\n",
        "  print('Confusion Matrix ',i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n",
        "  #f1_score(validation_generator.classes, y_pred, average='weighted', labels=np.unique(y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrxSI5ZqiaA_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#####ROC AUC Curve for YCbCr Model"
      ]
    },
    {
      "metadata": {
        "id": "ZTIBhmDDYx6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  from sklearn.metrics import auc, roc_curve\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66034\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = ycrcb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = Y_pred > i\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for YCRCB VGG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for YCRCB VGG')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for YCRCB VGG (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M44KWCnYfQY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###RGB COLOR Model Using VGG16 Network"
      ]
    },
    {
      "metadata": {
        "id": "ESpVYSjuKItO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dimensions of our images.\n",
        "img_width, img_height = 50, 50\n",
        "\n",
        "\n",
        "train_data_dir = 'idc_dataset/train/'\n",
        "validation_data_dir = 'idc_dataset/validation/'\n",
        "test_data_dir = 'idc_dataset/test/'\n",
        "nb_train_samples = 3519\n",
        "nb_validation_samples = 2272\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        " \n",
        "def auc_roc(y_true, y_pred):\n",
        "    # any tensorflow metric\n",
        "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
        "\n",
        "    # find all variables created for this metric\n",
        "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
        "\n",
        "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
        "    # They will be initialized for new session.\n",
        "    for v in metric_vars:\n",
        "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
        "\n",
        "    # force to update metric values\n",
        "    with tf.control_dependencies([update_op]):\n",
        "        value = tf.identity(value)\n",
        "        return value\n",
        "      \n",
        "#Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "#model_vgg16_conv.summary()\n",
        "\n",
        "#Create your own input format\n",
        "input = Input(shape=(50,50,3),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "x = Dense(1, activation='sigmoid',name= 'predictions')(x)\n",
        "\n",
        "\n",
        "#Create your own model \n",
        "rgb_model = Model(input=input, output=x)\n",
        "\n",
        "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
        "rgb_model.summary()\n",
        "\n",
        "rgb_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=1e-2),\n",
        "              metrics=['accuracy',auc_roc])\n",
        "\n",
        "my_callbacks = [EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')]\n",
        "\n",
        "# prepare data augmentation configuration\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1. / 255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    shuffle= False,\n",
        "    class_mode='binary')\n",
        "# fine-tune the model\n",
        "rgb_model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=my_callbacks,\n",
        "    nb_val_samples=nb_validation_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kQhy8MuSyYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix and Classification Report"
      ]
    },
    {
      "metadata": {
        "id": "tSs8o3Ap4s1P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,f1_score\n",
        "num_of_test_samples = 66034\n",
        "batch_size = 32\n",
        "#y_true = np.array([0] * 1000 + [1] * 1000)\n",
        "list1 = [0.2,0.29,0.4,0.5,0.7]\n",
        "for i in list1:\n",
        "  Y_pred = rgb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "  y_pred = Y_pred > i\n",
        "  # y_pred = np.argmax(Y_pred, axis=1)\n",
        "  # print(y_pred)\n",
        "  print('Confusion Matrix ',i)\n",
        "  print(confusion_matrix(test_generator.classes, y_pred))\n",
        "  print('Classification Report')\n",
        "  target_names = ['class0','class1']\n",
        "  print(classification_report(test_generator.classes, y_pred, target_names=target_names))\n",
        "  #f1_score(validation_generator.classes, y_pred, average='weighted', labels=np.unique(y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xIhbKK4S48Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### ROC AUC Curve For RGB Model"
      ]
    },
    {
      "metadata": {
        "id": "q6n-OSHx6kGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  from scipy import interp\n",
        "  from sklearn.metrics import auc, roc_curve\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0, 1, 100)\n",
        "  threshold_list = [0.29, 0.4, 0.5, 0.7]\n",
        "  num_of_test_samples = 66034\n",
        "  batch_size = 32\n",
        "  show_plot = True\n",
        "  \n",
        "  for i in threshold_list:\n",
        "           \n",
        "      # Compute ROC curve and area the curve\n",
        "      Y_pred = rgb_model.predict_generator(test_generator, num_of_test_samples // batch_size+1)\n",
        "      y_pred = Y_pred > i\n",
        "      \n",
        "      fpr, tpr, thresholds = roc_curve(test_generator.classes, y_pred)\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      tprs[-1][0] = 0.0\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      if show_plot:\n",
        "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "                 label='ROC fold for threshold = %f (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  std_auc = np.std(aucs)\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "\n",
        "  if show_plot:\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "             label='0.5', alpha=.8)\n",
        "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "             label= 'Mean ROC for RGB VGG (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "             lw=2, alpha=.8)\n",
        "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                     label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "    plt.xlim([-0.05, 1.05])\n",
        "    plt.ylim([-0.05, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic for RGB VGG')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "  \n",
        "  print('Mean ROC for RGB VGG (AUC = %0.8f +- %0.8f)' % (mean_auc, std_auc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bM5tZ7IbJtKw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Results"
      ]
    },
    {
      "metadata": {
        "id": "VMtKBmGIShxX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We combined and collated the results of the 8 implementations ( 4 basic CNN and 4 VGG 16) with 4 different thresholds in an google spreadsheet. The results for the runs can be viewed at the below link - \n",
        "\n",
        "**Statistics Link** - https://docs.google.com/spreadsheets/d/1lkJEwaH3MxT7WY453HKqdGRfYcBRB_22PzcjXKHtHr8/edit#gid=1976742492\n",
        "\n",
        "**Results Charts**-\n",
        "https://docs.google.com/drawings/d/1ht4hcZqAyTXQrVuZTASdkTQdNweJvgVvM_SKlsy03l0/edit?usp=sharing\n",
        "\n",
        "---\n",
        "**Hypothesis Validation -**\n",
        "\n",
        "As can be seen from the charts via the above link (RESULTS Charts) link, \n",
        "\n",
        "> 1. Recall scores vary for all the color spaces amongst different basic CNN and VGG 16 implementaions.\n",
        "\n",
        "> 2. Increase of 11.55% in Recall score when comparing best model (amongst all 8 implementations) to recall score for parent paper CNN implementation.\n",
        "\n",
        "> 3. Increase of 9.5% in Recall score when compared best VGG 16 YUV model against parent paper CNN implementation (which was also in YUV color space). \n",
        "\n",
        "> 4. RGB and YUV color space implementations for basic CNN and VGG 16 models performed better as compared to other colour feature extractors.\n"
      ]
    },
    {
      "metadata": {
        "id": "rUdZjVXWJv7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "obmwSIAoNFiy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "We were able to answer our hypothesis. We were able to answer the questions in the hypothesis by getting results which we had expected. \n",
        ">  1. By changing different color spaces, results do change in automatic detection of invasive ductal carcinoma in whole slide images. \n",
        ">  2. By comparing the best models for 4 different color spaces amongst normal CNN implementation and VGG 16 pre trained CNN implementations we found that change in recall scores varied by approximately 10 percent. \n",
        ">  3. We also found that RGB and YUV color spaces models for CNN and VGG 16 implementations performed better as compared to other colour spaces.\n",
        "\n",
        "For studies in similar projects, researchers should mustfirst assess the use case and identify which metric to use to compare the models. Our results can be comprehended by similar studies by validating their results using their dataset. For similar sized whole slide images, we propose to use normally equalised YUV color space pre processed images for better recall scores alongwith the deep neural network implementation.YUV is a compressed form of RGB color space, and hence runs faster on larger training and validation datasets with significantly little loss of accuracy.\n",
        "\n",
        "For future  studies, we would try different pre processing techniques including normalisation, extracting morphological features like nuclear textural and nuclear architectural and combine our study of identifying patches of IDC with grading fidderent grades of IDC Cancer.\n",
        "We would also validate our results with a different whole slide image dataset of images of significantly larger sizes. \n",
        "\n",
        "Finally, we learn that research in medical data has immense potential in solving some of the complex problems. Manual tasks which require human expertise can be learned by the machine with growing digital medical data for automation. This could lead to reducing the downtime in detecting malignant diseases. Alas, technology can help humans in medical field if the data is used judiciously."
      ]
    },
    {
      "metadata": {
        "id": "FenZDCFjJ0BX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "\n",
        "---\n",
        "\n",
        "Below are some of links which we had taken help from - "
      ]
    },
    {
      "metadata": {
        "id": "KNKDXJfsUUDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://ieeexplore.ieee.org/document/6108877/\n",
        "\n",
        "https://reader.elsevier.com/reader/sd/pii/S235291481630034X?token=89924E985A4CE229944C9F663567FAB61F3B7FE1DFDF463E6D1527A0AD30260CDF21C944648E0B3D817348A9D4D535D8\n",
        "\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5453426/\n",
        "\n",
        "http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
        "\n",
        "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb\n",
        "\n",
        "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "\n",
        "https://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html#ga397ae87e1288a81d2363b61574eb8cab\n",
        "\n",
        "https://www.cancer.org/treatment/understanding-your-diagnosis/tests/testing-biopsy-and-cytology-specimens-for-cancer/what-doctors-look-for.html\n",
        "\n",
        "https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2\n",
        "\n",
        "https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\n",
        "\n",
        "https://github.com/keras-team/keras/issues/4465\n",
        "\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1634843/\n",
        "\n",
        "https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
        "\n",
        "https://github.com/keras-team/keras/issues/1006\n",
        "\n",
        "https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
        "\n",
        "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
        "\n",
        "https://isaacchanghau.github.io/post/activation_functions/"
      ]
    }
  ]
}